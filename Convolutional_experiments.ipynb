{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# DLE305 \n",
    "The goal of this notebook is to experiment with convolutional neural networks and update the code to be more readable.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code cell performs the task of installing the necessary libraries using pip. The > null 2>&1 part redirects both the standard output and standard error to null, effectively silencing any output or error messages."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install tensorflow > null 2>&1\n",
    "!pip install seaborn > null 2>&1\n",
    "!pip install numpy > null 2>&1\n",
    "!pip install pillow > null 2>&1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code cell performs the task of importing the necessary libraries."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras import layers, models, optimizers\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code cell performs the task of defining a function `pixels_from_path` that resizes an image to a specified size and converts it to a NumPy array.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "IMG_SIZE = (94, 125)\n",
    "def pixels_from_path(file_path):\n",
    "    im = Image.open(file_path)\n",
    "    im = im.resize(IMG_SIZE)\n",
    "    np_im = np.array(im)\n",
    "    return np_im"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code cell performs the task of using the glob module to get the file paths of all the images in the cats folder."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "glob.glob('cats/*')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code cell performs the task of counting the occurrences of different image shapes in the cats folder. It uses the pixels_from_path function to get the shape of each image and stores the counts in a defaultdict."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "shape_counts = defaultdict(int)\n",
    "for i, cat in enumerate(glob.glob('cats/*')[:1000]):\n",
    "    if i%100==0:\n",
    "        print(i)\n",
    "    img_shape = pixels_from_path(cat).shape\n",
    "    shape_counts[str(img_shape)]= shape_counts[str(img_shape)]+ 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code cell performs the task of sorting the shape_counts dictionary items by their counts in descending order and storing the sorted items in the shape_items list."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "shape_items = list(shape_counts.items())\n",
    "shape_items.sort(key = lambda x: x[1])\n",
    "shape_items.reverse()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code cell sets up some parameters for the data processing and model training:"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 10% of the data will automatically be used for validation\n",
    "validation_size = 0.1\n",
    "img_size = IMG_SIZE # resize images to be 374x500 (most common shape)\n",
    "num_channels = 3 # RGB\n",
    "sample_size = 8192 #We'll use 8192 pictures (2**13)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code cell performs the task of loading the training data by counting the number of image files in the cats folder.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "len(glob.glob('cats/*'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code cell tests the function pixels_from_path by getting the shape of the image at index 5 in the cats folder."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pixels_from_path(glob.glob('cats/*')[5]).shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code block loads and verifies the shape of an image from the cats folder. It uses the glob module to get file paths, the PIL library to open and resize images, and NumPy to convert images to arrays."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "IMG_SIZE = (94, 125)\n",
    "\n",
    "def pixels_from_path(file_path):\n",
    "    try:\n",
    "        im = Image.open(file_path)\n",
    "        im = im.resize(IMG_SIZE)\n",
    "        np_im = np.array(im)\n",
    "        return np_im\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Check the file paths\n",
    "file_paths = glob.glob('cats/*')\n",
    "print(f\"Found {len(file_paths)} files.\")\n",
    "\n",
    "# Verify the image file at index 5\n",
    "if len(file_paths) > 5:\n",
    "    image_shape = pixels_from_path(file_paths[5])\n",
    "    if image_shape is not None:\n",
    "        print(f\"Image shape: {image_shape.shape}\")\n",
    "    else:\n",
    "        print(\"Failed to process the image.\")\n",
    "else:\n",
    "    print(\"Not enough files found.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code block loads the training data for cats and dogs by using the pixels_from_path function to process the images and convert them to NumPy arrays. It loads the first 2048 images from the cats and dogs folders."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "SAMPLE_SIZE = 2048\n",
    "print(\"loading training cat images...\")\n",
    "cat_train_set = np.asarray([pixels_from_path(cat) for cat in glob.glob('cats/*')[:SAMPLE_SIZE]])\n",
    "print(\"loading training dog images...\")\n",
    "dog_train_set = np.asarray([pixels_from_path(dog) for dog in glob.glob('dogs/*')[:SAMPLE_SIZE]])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code block loads the validation data for cats and dogs by using the pixels_from_path function to process the images and convert them to NumPy arrays. It loads the last 512 images from the cats and dogs folders."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "valid_size = 512\n",
    "print(\"loading validation cat images...\")\n",
    "cat_valid_set = np.asarray([pixels_from_path(cat) for cat in glob.glob('cats/*')[-valid_size:]])\n",
    "print(\"loading validation dog images...\")\n",
    "dog_valid_set = np.asarray([pixels_from_path(dog) for dog in glob.glob('dogs/*')[-valid_size:]])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code block is for checking the shape of the training data"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "x_train = np.concatenate([cat_train_set, dog_train_set])\n",
    "labels_train = np.asarray([1 for _ in range(SAMPLE_SIZE)]+[0 for _ in range(SAMPLE_SIZE)])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code block is for loading the validation data"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "x_valid = np.concatenate([cat_valid_set, dog_valid_set])\n",
    "labels_valid = np.asarray([1 for _ in range(valid_size)]+[0 for _ in range(valid_size)])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code block is for checking the shape of the training data"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "x_train.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code block is for checking the shape of the training data"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "labels_train.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run of the Mill MLP"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code block is for creating a model"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "total_pixels = img_size[0] *img_size[1] * 3\n",
    "fc_size = 512\n",
    "\n",
    "inputs = keras.Input(shape=(img_size[1], img_size[0],3), name='ani_image')\n",
    "x = layers.Flatten(name = 'flattened_img')(inputs) #turn image to vector.\n",
    "\n",
    "x = layers.Dense(fc_size, activation='relu', name='first_layer')(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid', name='class')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Compile the model\n",
    "### Changes:\n",
    "- lr was not recognised so it was changed to learning_rate"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code block is for compiling the model"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "customAdam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=customAdam,  # Optimizer\n",
    "              # Loss function to minimize\n",
    "              loss=\"mean_squared_error\",\n",
    "              # List of metrics to monitor\n",
    "              metrics=[\"binary_crossentropy\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Changes:\n",
    "- Reshape labels_train to match the model's output shape\n",
    "\n",
    "this code block is for fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Reshape labels to match the output shape\n",
    "labels_train = labels_train.reshape(-1, 1)\n",
    "labels_valid = labels_valid.reshape(-1, 1)\n",
    "\n",
    "print('Fit model on training data')\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    labels_train,\n",
    "                    batch_size=32,\n",
    "                    shuffle=True,\n",
    "                    epochs=10,\n",
    "                    validation_data=(x_valid, labels_valid))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Convolutional Layer'\n",
    "### Changes:\n",
    "- lr was not recognised so it was changed to learning_rate\n",
    "this code block is for creating a model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fc_layer_size = 128\n",
    "img_size = IMG_SIZE\n",
    "\n",
    "conv_inputs = keras.Input(shape=(img_size[1], img_size[0],3), name='ani_image')\n",
    "conv_layer = layers.Conv2D(24, kernel_size=3, activation='relu')(conv_inputs)\n",
    "conv_layer = layers.MaxPool2D(pool_size=(2,2))(conv_layer)\n",
    "conv_x = layers.Flatten(name = 'flattened_features')(conv_layer) #turn image to vector.\n",
    "\n",
    "conv_x = layers.Dense(fc_layer_size, activation='relu', name='first_layer')(conv_x)\n",
    "conv_x = layers.Dense(fc_layer_size, activation='relu', name='second_layer')(conv_x)\n",
    "conv_outputs = layers.Dense(1, activation='sigmoid', name='class')(conv_x)\n",
    "\n",
    "conv_model = keras.Model(inputs=conv_inputs, outputs=conv_outputs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Changes:\n",
    "- lr was not recognised so it was changed to learning_rate\n",
    "\n",
    "this code block is for compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "customAdam = keras.optimizers.Adam(learning_rate=1e-6)\n",
    "conv_model.compile(optimizer=customAdam,  # Optimizer\n",
    "              # Loss function to minimize\n",
    "              loss=\"binary_crossentropy\",\n",
    "              # List of metrics to monitor\n",
    "              metrics=[\"binary_crossentropy\",\"mean_squared_error\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Changes:\n",
    "- Reshape labels_train to match the model's output shape"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print('# Fit model on training data')\n",
    "\n",
    "# Reshape labels_train to match the model's output shape\n",
    "labels_train = labels_train.reshape(-1, 1)\n",
    "labels_valid = labels_valid.reshape(-1, 1)\n",
    "\n",
    "history = conv_model.fit(x_train, \n",
    "                    labels_train, # we pass it the labels\n",
    "                    # If the model is taking forever to train, make this bigger\n",
    "                    # If it is taking forever to load for the first epoch, make this smaller\n",
    "                    batch_size=32, \n",
    "                    shuffle=True,\n",
    "                    epochs=5,\n",
    "                    # We pass it validation data to\n",
    "                    # monitor loss and metrics\n",
    "                    # at the end of each epoch\n",
    "                    validation_data=(x_valid, labels_valid))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Changes:\n",
    "- Flatten the predictions to match the shape of the labels\n",
    "\n",
    "this code block is for calculating the correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure preds and labels_valid are 1-dimensional\n",
    "preds = np.asarray(preds).flatten()\n",
    "labels_valid = np.asarray(labels_valid).flatten()\n",
    "\n",
    "# Predict values\n",
    "preds = conv_model.predict(x_valid)\n",
    "preds = np.asarray([pred[0] for pred in preds])\n",
    "\n",
    "# Calculate correlation coefficient\n",
    "np.corrcoef(preds, labels_valid)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code block is for creating a scatter plot"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "sns.scatterplot(x= preds, y= labels_valid)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code block is for calculating the mean of the predictions"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cat_quantity = sum(labels_valid)\n",
    "\n",
    "for i in range(1,10):\n",
    "    print('threshold :'+str(.1*i))\n",
    "    print(sum(labels_valid[preds > .1*i])/labels_valid[preds > .1*i].shape[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "this code block is for saving the model"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(preds.mean())\n",
    "print(preds[labels_valid == 0].mean())\n",
    "print(preds[labels_valid == 1].mean())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Bigger Convolutional Model"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code block is for creating a model"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fc_layer_size = 256\n",
    "img_size = IMG_SIZE\n",
    "\n",
    "conv_inputs = keras.Input(shape=(img_size[1], img_size[0],3), name='ani_image')\n",
    "conv_layer = layers.Conv2D(48, kernel_size=3, activation='relu')(conv_inputs)\n",
    "conv_layer = layers.MaxPool2D(pool_size=(2,2))(conv_layer)\n",
    "\n",
    "conv_layer = layers.Conv2D(48, kernel_size=3, activation='relu')(conv_layer)\n",
    "conv_layer = layers.MaxPool2D(pool_size=(2,2))(conv_layer)\n",
    "\n",
    "conv_x = layers.Flatten(name = 'flattened_features')(conv_layer) #turn image to vector.\n",
    "\n",
    "conv_x = layers.Dense(fc_layer_size, activation='relu', name='first_layer')(conv_x)\n",
    "conv_x = layers.Dense(fc_layer_size, activation='relu', name='second_layer')(conv_x)\n",
    "conv_outputs = layers.Dense(1, activation='sigmoid', name='class')(conv_x)\n",
    "\n",
    "conv_model = keras.Model(inputs=conv_inputs, outputs=conv_outputs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Changes:\n",
    "- lr was not recognised so it was changed to learning_rate"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code block is for compiling the model"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "customAdam = keras.optimizers.Adam(learning_rate=1e-6)\n",
    "conv_model.compile(optimizer=customAdam,  # Optimizer\n",
    "              # Loss function to minimize\n",
    "              loss=\"binary_crossentropy\",\n",
    "              # List of metrics to monitor\n",
    "              metrics=[\"binary_crossentropy\",\"mean_squared_error\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Changes:\n",
    "- Reshape labels_train to match the model's output shape\n",
    "\n",
    "This code block is for fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Fit model on training data\n",
    "print('# Fit model on training data')\n",
    "\n",
    "# Reshape labels_train to match the model's output shape\n",
    "labels_train = labels_train.reshape(-1, 1)\n",
    "labels_valid = labels_valid.reshape(-1, 1)\n",
    "\n",
    "history = conv_model.fit(x_train, \n",
    "                    labels_train, # we pass it the labels\n",
    "                    # If the model is taking forever to train, make this bigger\n",
    "                    # If it is taking forever to load for the first epoch, make this smaller\n",
    "                    batch_size=64, \n",
    "                    shuffle=True,\n",
    "                    epochs=15,\n",
    "                    # We pass it validation data to\n",
    "                    # monitor loss and metrics\n",
    "                    # at the end of each epoch\n",
    "                    validation_data=(x_valid, labels_valid))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code block is for calculating the correlation coefficient"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure preds and labels_valid are 1-dimensional\n",
    "preds = np.asarray(preds).flatten()\n",
    "labels_valid = np.asarray(labels_valid).flatten()\n",
    "\n",
    "# Predict values\n",
    "preds = conv_model.predict(x_valid)\n",
    "preds = np.asarray([pred[0] for pred in preds])\n",
    "\n",
    "# Calculate correlation coefficient\n",
    "np.corrcoef(preds, labels_valid)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code block is for creating a scatter plot"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sns.scatterplot(x= preds, y= labels_valid)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code block is for calculating the mean of the predictions"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(preds.mean())\n",
    "print(preds[labels_valid == 0].mean())\n",
    "print(preds[labels_valid == 1].mean())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code block calculates and prints the proportion of positive labels (i.e., `labels_valid` equal to 1) for different threshold values applied to the predictions (`preds`). Here's a step-by-step explanation:\n",
    "\n",
    "1. `cat_quantity = sum(labels_valid)`: This line calculates the total number of positive labels in `labels_valid`.\n",
    "\n",
    "2. The `for` loop iterates over a range of threshold values from 0.1 to 0.9 (in increments of 0.1).\n",
    "\n",
    "3. Inside the loop:\n",
    "   - `print('threshold :'+str(.1*i))`: This line prints the current threshold value.\n",
    "   - `print(sum(labels_valid[preds > .1*i])/labels_valid[preds > .1*i].shape[0])`: This line calculates and prints the proportion of positive labels for predictions greater than the current threshold. It does this by:\n",
    "     - Filtering `labels_valid` to include only those entries where the corresponding `preds` value is greater than the current threshold (`.1*i`).\n",
    "     - Summing the filtered `labels_valid` values to get the count of positive labels.\n",
    "     - Dividing this count by the total number of filtered entries to get the proportion of positive labels.\n",
    "\n",
    "The output will show how the proportion of positive labels changes as the threshold increases."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cat_quantity = sum(labels_valid)\n",
    "\n",
    "for i in range(1,10):\n",
    "    print('threshold :'+str(.1*i))\n",
    "    print(sum(labels_valid[preds > .1*i])/labels_valid[preds > .1*i].shape[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def animal_pic(index):\n",
    "    return Image.fromarray(x_valid[index])\n",
    "def cat_index(index):\n",
    "    return conv_model.predict(np.asarray([x_valid[124]]))[0][0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Changes:\n",
    "- added .keras to the file name"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "conv_model.save('conv_model_big.keras')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "index = 600\n",
    "print(\"probability of being a cat: {}\".format(cat_index(index)))\n",
    "animal_pic(index)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "conv_model.predict(np.asarray([x_valid[124]]))[0][0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Changes:\n",
    "- added .keras to the file name"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "big_model = keras.models.load_model('conv_model_big.keras')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huge Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fc_layer_size = 256\n",
    "img_size = IMG_SIZE\n",
    "\n",
    "conv_inputs = keras.Input(shape=(img_size[1], img_size[0],3), name='ani_image')\n",
    "conv_layer = layers.Conv2D(128, kernel_size=3, activation='relu')(conv_inputs)\n",
    "conv_layer = layers.MaxPool2D(pool_size=(2,2))(conv_layer)\n",
    "\n",
    "conv_layer = layers.Conv2D(128, kernel_size=3, activation='relu')(conv_layer)\n",
    "conv_layer = layers.MaxPool2D(pool_size=(2,2))(conv_layer)\n",
    "\n",
    "conv_x = layers.Flatten(name = 'flattened_features')(conv_layer) #turn image to vector.\n",
    "\n",
    "conv_x = layers.Dense(fc_layer_size, activation='relu', name='first_layer')(conv_x)\n",
    "conv_x = layers.Dense(fc_layer_size, activation='relu', name='second_layer')(conv_x)\n",
    "conv_outputs = layers.Dense(1, activation='sigmoid', name='class')(conv_x)\n",
    "\n",
    "huge_conv_model = keras.Model(inputs=conv_inputs, outputs=conv_outputs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Changes:\n",
    "- lr was not recognised so it was changed to learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "customAdam = keras.optimizers.Adam(learning_rate=1e-6)\n",
    "huge_conv_model.compile(optimizer=customAdam,  # Optimizer\n",
    "              # Loss function to minimize\n",
    "              loss=\"binary_crossentropy\",\n",
    "              # List of metrics to monitor\n",
    "              metrics=[\"binary_crossentropy\",\"mean_squared_error\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Reshape labels_train to match the model's output shape\n",
    "labels_train = labels_train.reshape(-1, 1)\n",
    "labels_valid = labels_valid.reshape(-1, 1)\n",
    "\n",
    "print('# Fit model on training data')\n",
    "\n",
    "history = huge_conv_model.fit(x_train, \n",
    "                    labels_train, #we pass it th labels\n",
    "                    #If the model is taking forever to train, make this bigger\n",
    "                    #If it is taking forever to load for the first epoch, make this smaller\n",
    "                    batch_size=64, \n",
    "                    shuffle = True,\n",
    "                    epochs=5,\n",
    "                    # We pass it validation data to\n",
    "                    # monitor loss and metrics\n",
    "                    # at the end of each epoch\n",
    "                    validation_data=(x_valid, labels_valid))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Changes:\n",
    "- Flatten the predictions to match the shape of the labels"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "# Ensure preds and labels_valid are 1-dimensional\n",
    "preds = np.asarray(preds).flatten()\n",
    "labels_valid = np.asarray(labels_valid).flatten()\n",
    "\n",
    "preds = huge_conv_model.predict(x_valid)\n",
    "preds = np.asarray([pred[0] for pred in preds])\n",
    "np.corrcoef(preds, labels_valid)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(preds.mean())\n",
    "print(preds[labels_valid == 0].mean())\n",
    "print(preds[labels_valid == 1].mean())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cat_quantity = sum(labels_valid)\n",
    "\n",
    "for i in range(1,10):\n",
    "    print('threshold :'+str(.1*i))\n",
    "    print(sum(labels_valid[preds > .1*i])/labels_valid[preds > .1*i].shape[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Changes:\n",
    "- added .keras to the file name"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "huge_conv_model.save('conv_model_huge_e13.keras')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "big_model = keras.models.load_model('conv_model_huge_e13.keras')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "preds = big_model.predict(x_valid)\n",
    "preds = np.asarray([pred[0] for pred in preds])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sum(labels_valid)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for i in range(1,10):\n",
    "    t = .1*i\n",
    "    print(\"{:.1f}:\".format(t))\n",
    "    tp = (preds > t)&(labels_valid==1)\n",
    "    tn = (preds <= t)&(labels_valid==0)\n",
    "    print(np.sum(np.where(tp|tn, 1, 0))/1024.)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Reshape labels_train to match the model's output shape\n",
    "labels_train = labels_train.reshape(-1, 1)\n",
    "labels_valid = labels_valid.reshape(-1, 1)\n",
    "\n",
    "print('# Fit model on training data')\n",
    "\n",
    "history = big_model.fit(x_train, \n",
    "                    labels_train, #we pass it th labels\n",
    "                    #If the model is taking forever to train, make this bigger\n",
    "                    #If it is taking forever to load for the first epoch, make this smaller\n",
    "                    batch_size=64, \n",
    "                    shuffle = True,\n",
    "                    epochs=10,\n",
    "                    # We pass it validation data to\n",
    "                    # monitor loss and metrics\n",
    "                    # at the end of each epoch\n",
    "                    validation_data=(x_valid, labels_valid))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "preds = big_model.predict(x_valid)\n",
    "preds = np.asarray([pred[0] for pred in preds])\n",
    "for i in range(1,10):\n",
    "    t = .1*i\n",
    "    print(\"{:.1f}:\".format(t))\n",
    "    tp = (preds > t)&(labels_valid==1)\n",
    "    tn = (preds <= t)&(labels_valid==0)\n",
    "    print(np.sum(np.where(tp|tn, 1, 0))/1024.)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "big_model.save('conv_model_big_e19.keras')",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
